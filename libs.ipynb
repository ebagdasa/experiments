{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import zlib\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import multiprocessing\n",
    "from subprocess import run, PIPE\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "client = MongoClient()\n",
    "db = client.libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./libs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('./libs/dependencies-1.0.0-2017-06-15.csv')\n",
    "reader = csv.reader(f, delimiter=',')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "versions_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir('./libs'):\n",
    "    if '.csv' in file:\n",
    "        with open('./libs/{0}'.format(file)) as g:\n",
    "            reader = csv.reader(g, delimiter=',')    \n",
    "            heads = next(reader)\n",
    "            example = next(reader)\n",
    "            print('{0}: {1} \\n {2} \\n\\n'.format(file, heads, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this release you will find data about software distributed and/or crafted publicly on the Internet. You will find information about its development, its distribution and its relationship with other software included as a dependency. You will not find any information about the individuals who create and maintain these projects.\n",
    "\n",
    "Further information and documentation on this data set can be found at https://libraries.io/data\n",
    "\n",
    "For enquiries please contact data@libraries.io\n",
    "\n",
    "This dataset contains seven files:\n",
    "\n",
    "## Projects\n",
    "A project is a piece of software available on any one of the 33 package managers supported by Libraries.io.\n",
    "\n",
    "## Versions\n",
    "A Libraries.io version is an immutable published version of a Project from a package manager. Not all package managers have a concept of publishing versions, often relying directly on tags/branches from a revision control tool.\n",
    "\n",
    "## Tags\n",
    "A tag is equivalent to a tag in a revision control system. Tags are sometimes used instead of Versions where a package manager does not use the concept of versions. Tags are often semantic version numbers.\n",
    "\n",
    "## Dependencies\n",
    "Dependencies describe the relationship between a project and the software it builds upon. Dependencies belong to Version. Each Version can have different sets of dependencies. Dependencies point at a specific Version or range of versions of other projects.\n",
    "\n",
    "## Repositories\n",
    "A Libraries.io repository represents a publicly accessible source code repository from either github.com, gitlab.com or bitbucket.org. Repositories are distinct from Projects, they are not distributed via a package manager and typically an application for end users rather than component to build upon.\n",
    "\n",
    "## Repository dependencies\n",
    "A repository dependency is a dependency upon a Version from a package manager has been specified in a manifest file, either as a manually added dependency committed by a user or listed as a generated dependency listed in a lockfile that has been automatically generated by a package manager and committed.\n",
    "\n",
    "## Projects with related Repository fields\n",
    "This is an alternative projects export that denormalizes a projects related source code repository inline to reduce the need to join between two data sets.\n",
    "\n",
    "# License\n",
    "This dataset is released under the Creative Commons Attribution-ShareAlike 4.0 International License.\n",
    "\n",
    "This license provides the user with the freedom to use, adapt and redistribute this data. In return the user must publish any derivative work under a similarly open license, attributing Libraries.io as a data source. The full text of the license is included in the data.\n",
    "\n",
    "# Access, Attribution and Citation\n",
    "The dataset is available to download from Zenodo at https://zenodo.org/record/808273.\n",
    "\n",
    "Please attribute Libraries.io as a data source by including the words ‘Includes data from Libraries.io’ and reference the Digital Object identifier: 10.5281/Zenodo.808273."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./libs/{0}'.format('dependencies-1.0.0-2017-06-15.csv')) as g:\n",
    "        reader = csv.reader(g, delimiter=',')    \n",
    "        headers = next(reader)\n",
    "        print(headers)\n",
    "        for entry in reader:\n",
    "            e_dict = dict()\n",
    "            for i, field in enumerate(entry):\n",
    "                e_dict[header[i]] = field\n",
    "            print(e_dict)\n",
    "            break\n",
    "#             db.dependencies.insert_one(e_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./libs/{0}'.format('projects-1.0.0-2017-06-15.csv')) as g:\n",
    "        reader = csv.reader(g, delimiter=',')    \n",
    "        i=0\n",
    "        for a in reader:\n",
    "            print(a)\n",
    "            break\n",
    "            i+=1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet size:\n",
    "\n",
    "projects: 2215454\n",
    "dependencies: 51239987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./libs/{0}'.format('dependencies-1.0.0-2017-06-15.csv')) as g:\n",
    "        reader = csv.reader(g, delimiter=',')    \n",
    "        headers = next(reader)\n",
    "#         db.dependencies.drop()\n",
    "        thousand = 0\n",
    "        t_list = list()\n",
    "        for entry in reader:\n",
    "            e_dict = dict()\n",
    "            for i, field in enumerate(entry):\n",
    "                e_dict[headers[i]] = field\n",
    "            thousand += 1\n",
    "            t_list.append(e_dict)\n",
    "            if thousand % 1000 == 0:\n",
    "                thousand=0\n",
    "                \n",
    "                db.dependencies.insert_many(t_list)\n",
    "                t_list = list()\n",
    "        \n",
    "        db.dependencies.insert_many(t_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./libs/{0}'.format('projects-1.0.0-2017-06-15.csv')) as g:\n",
    "        reader = csv.reader(g, delimiter=',')    \n",
    "        headers = next(reader)\n",
    "        db.projects.drop()\n",
    "        thousand = 0\n",
    "        t_list = list()\n",
    "        for entry in reader:\n",
    "            e_dict = dict()\n",
    "            for i, field in enumerate(entry):\n",
    "                e_dict[headers[i]] = field\n",
    "            thousand += 1\n",
    "            t_list.append(e_dict)\n",
    "            if thousand % 1000 == 0:\n",
    "                thousand=0\n",
    "                \n",
    "                db.projects.insert_many(t_list)\n",
    "                t_list = list()\n",
    "        \n",
    "        db.projects.insert_many(t_list)\n",
    "print(db.projects.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### things to do next\n",
    "\n",
    "1. decide how to split the data to users and items\n",
    "2. do filtering of dependencies\n",
    "3. implement the simple MF\n",
    "4. implement the recall or AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.dependencies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iter(cursor, batch_size=10000):\n",
    "    mylist = list()\n",
    "    it = 0\n",
    "    for item in cursor:\n",
    "        it +=1\n",
    "        mylist.append(item)\n",
    "        if it%batch_size==0:\n",
    "            yield mylist\n",
    "            it=0\n",
    "            mylist= list()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deps_proj = dict()\n",
    "proj_num = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = db.dependencies.find()\n",
    "batch_size=10000\n",
    "cursor.batch_size(batch_size=batch_size)\n",
    "my_iter = data_iter(cursor, batch_size)\n",
    "for entry in cursor:\n",
    "    deps_proj[entry['Project ID']] = deps_proj.get(entry['Project ID'],0) + 1\n",
    "    dep_proj_name = entry['Dependency Project ID'] if entry.get('Dependency Project ID',False) else \\\n",
    "        (entry['Dependency Name']  + '_' +  entry['Dependency Kind']  + '_' +  entry['Dependency Platform']  \\\n",
    "        + '_' + entry['Dependency Project ID']  + '_' +  entry['Dependency Requirements'])\n",
    "    proj_num[dep_proj_name] = proj_num.get(dep_proj_name, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "\n",
    "projects: 775580\n",
    "\n",
    "Referenced projects: 420872\n",
    "\n",
    "Total interactions: 51239986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.average(list(proj_num.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(list(proj_num.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(proj_num.keys())[213331]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = set(proj_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = a.union(set(deps_proj.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(proj_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = db.python.find()\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match_id = dict()\n",
    "\n",
    "repo_pro = dict()\n",
    "repo_dep = dict()\n",
    "\n",
    "class idgiver:\n",
    "    def __init__(self):\n",
    "        self.iterr=0\n",
    "        self.repo_pro2 = dict()\n",
    "        self.repo_dep2 = dict()\n",
    "        \n",
    "    def get_id(self, g_id):\n",
    "        if not match_id.get(g_id, False):\n",
    "            match_id[g_id] = self.iterr\n",
    "            self.iterr+=1\n",
    "        return match_id[g_id]\n",
    "idgive = idgiver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = db.js6.find()\n",
    "for entry in curs:\n",
    "    p = entry['project']\n",
    "    d = entry['depenendent']\n",
    "    repo_pro[p]= repo_pro.get(p, 0) + 1\n",
    "    repo_dep[d]= repo_dep.get(d, 0) + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = db.js6.find()\n",
    "for entry in curs:\n",
    "    if repo_pro[entry['project']]>=20 and repo_dep[entry['depenendent']]>=20 and \\\n",
    "        repo_pro[entry['project']]<=100 and repo_dep[entry['depenendent']]<=2000:\n",
    "        \n",
    "        p = entry['project']\n",
    "        d = entry['depenendent']\n",
    "        idgive.repo_pro2[p]= idgive.repo_pro2.get(p, 0) + 1\n",
    "        idgive.repo_dep2[d]= idgive.repo_dep2.get(d, 0) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(idgive.repo_pro2))\n",
    "print(len(idgive.repo_dep2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curs = db.js6.find()\n",
    "edges_list = list()\n",
    "for entry in curs:\n",
    "\n",
    "    if idgive.repo_pro2.get(entry['project'],0)>=20 and idgive.repo_dep2.get(entry['depenendent'],0)>=20:\n",
    "        p = idgive.get_id(entry['project'])\n",
    "        d = idgive.get_id(entry['depenendent'])\n",
    "        edges_list.append((p,d))\n",
    "        edges_list.append((d,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idgive.iterr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G=nx.Graph()\n",
    "G.add_edges_from(edges_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for edge in G.edges():\n",
    "    G[edge[0]][edge[1]]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gx = Graph(G, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gx.preprocess_transition_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Gx.self.alias_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(match_id, 'match_id')\n",
    "save_obj(id_to_name, 'id_to_name')\n",
    "save_obj(edges_list, 'edges_list')\n",
    "save_obj(repo_count, 'repo_count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match_id = load_obj('match_id')\n",
    "id_to_name = load_obj('id_to_name')\n",
    "edges_list = load_obj('edges_list')\n",
    "repo_count = load_obj('repo_count')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edges_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [ k for (k,_) in edges_list]\n",
    "row = [ w for (_,w) in edges_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sha_pe = idgive.iterr\n",
    "print(sha_pe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mid results\n",
    "\n",
    "sp_m - sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = len(col)*[1]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_m = sparse.coo_matrix((data, (row,col)), shape=(sha_pe,sha_pe)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_m = sparse.load_npz('obj/sparse.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse.save_npz('obj/sparse4', sp_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_m.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 1.0\n",
    "q=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sp_m, p, q are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class randomWalks:\n",
    "    def __init__(self, sp_m, p, q):\n",
    "        self.sp_m = sp_m\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.random_walks = list()\n",
    "        self.transition = dict()\n",
    "rW = randomWalks(sp_m, p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "cpu_count = mp.cpu_count()\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_transition_prob(adj_mat_csr_sparse, p, q):\n",
    "\n",
    "    transition={}\n",
    "    num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "    indices=adj_mat_csr_sparse.indices\n",
    "    indptr=adj_mat_csr_sparse.indptr\n",
    "    data=adj_mat_csr_sparse.data\n",
    "    #Precompute the transition matrix in advance\n",
    "    for t in range(num_nodes):#t is row index\n",
    "        if t%10==0:\n",
    "            print('iteration: {0} '.format(t))\n",
    "        for v in indices[indptr[t]:indptr[t+1]]:#i.e  possible next ndoes from t\n",
    "            pi_vx_indices=indices[indptr[v]:indptr[v+1]]#i.e  possible next ndoes from v\n",
    "            pi_vx_values = np.array([alpha(p,q,t,x,adj_mat_csr_sparse) for x in pi_vx_indices])\n",
    "            pi_vx_values=pi_vx_values*data[indptr[v]:indptr[v+1]]\n",
    "\n",
    "            pi_vx_values=pi_vx_values/np.sum(pi_vx_values)\n",
    "            #now, we have normalzied transion probabilities for v traversed from t\n",
    "            #the probabilities are stored as a sparse vector. \n",
    "            transition[t,v]=(pi_vx_indices,pi_vx_values)\n",
    "\n",
    "    return transition\n",
    "\n",
    "\n",
    "def alpha(p,q,t,x,adj_mat_csr_sparse):\n",
    "    if t==x:\n",
    "        return 1.0/p\n",
    "    elif adj_mat_csr_sparse[t,x]>0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0/q\n",
    "    \n",
    "def generate_random_walks(dfasdfds):\n",
    "    adj_mat_csr_sparse=rW.sp_m\n",
    "    transition=rW.transition\n",
    "    random_walk_length=100\n",
    "    random_walks=[]\n",
    "    \n",
    "    num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "    indices=adj_mat_csr_sparse.indices\n",
    "    indptr=adj_mat_csr_sparse.indptr\n",
    "    data=adj_mat_csr_sparse.data\n",
    "    #get random walks\n",
    "    for u in range(num_nodes):\n",
    "        if len(indices[indptr[u]:indptr[u+1]]) !=0:\n",
    "            #first move is just depends on weight\n",
    "            possible_next_node=indices[indptr[u]:indptr[u+1]]\n",
    "            weight_for_next_move=data[indptr[u]:indptr[u+1]]#i.e  possible next ndoes from u\n",
    "            weight_for_next_move=weight_for_next_move.astype(np.float32)/np.sum(weight_for_next_move)\n",
    "            first_walk=np.random.choice(possible_next_node, 1, p=weight_for_next_move)\n",
    "            random_walk=[u,first_walk[0]]\n",
    "            for i in range(random_walk_length-2):\n",
    "                cur_node = random_walk[-1]\n",
    "                precious_node=random_walk[-2]\n",
    "#                 print(precious_node)\n",
    "#                 print(cur_node)\n",
    "                (pi_vx_indices,pi_vx_values)=transition[precious_node,cur_node]\n",
    "                next_node=np.random.choice(pi_vx_indices, 1, p=pi_vx_values)\n",
    "                random_walk.append(next_node[0])\n",
    "            random_walks.append(random_walk)\n",
    "\n",
    "    return random_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition = compute_transition_prob(sp_m, p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rW.transition = transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(transition, 'transition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition = load_obj('transition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_walk_length=100\n",
    "\n",
    "pool = mp.Pool(cpu_count)\n",
    "callback = pool.map(generate_random_walks, range(cpu_count))\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_walks = []\n",
    "for entry in callback:\n",
    "    random_walks.extend(entry) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_random_walks=np.array(random_walks,dtype=np.int32)\n",
    "np.savez('obj/np_random_walks4',np_random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_random_walks = np.load('obj/np_random_walks.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj_mat_csr_sparse =sp_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(np_random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "context_size=16\n",
    "batch_size = None\n",
    "embedding_size = 200 # Dimension of the embedding vector.\n",
    "num_sampled = 64 # Number of negative examples to sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Parameters to learn\n",
    "node_embeddings = tf.Variable(tf.random_uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(tf.truncated_normal([num_nodes, embedding_size],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "softmax_biases = tf.Variable(tf.zeros([num_nodes]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.name_scope(\"context_node\") as scope:\n",
    "    #context nodes to each input node in the batch (e.g [[1,2],[4,6],[5,7]] where batch_size = 3,context_size=3)\n",
    "    train_context_node= tf.placeholder(tf.int32, shape=[batch_size,context_size],name=\"context_node\")\n",
    "    #orgenize prediction labels (skip-gram model predicts context nodes (i.e labels) given a input node)\n",
    "    #i.e make [[1,2,4,6,5,7]] given context above. The redundant dimention is just for restriction on tensorflow API.\n",
    "    train_context_node_flat=tf.reshape(train_context_node,[-1,1])\n",
    "with tf.name_scope(\"input_node\") as scope:\n",
    "    #batch input node to the network(e.g [2,1,3] where batch_size = 3)\n",
    "    train_input_node= tf.placeholder(tf.int32, shape=[batch_size],name=\"input_node\")\n",
    "    #orgenize input as flat. i.e we want to make [2,2,2,1,1,1,3,3,3] given the  input nodes above\n",
    "    input_ones=tf.ones_like(train_context_node)\n",
    "    train_input_node_flat=tf.reshape(tf.multiply(input_ones,tf.reshape(train_input_node,[-1,1])),[-1])\n",
    "\n",
    "\n",
    "# # Model.\n",
    "# with tf.name_scope(\"loss\") as scope:\n",
    "#     # Look up embeddings for words.\n",
    "#     node_embed = tf.nn.embedding_lookup(node_embeddings, train_input_node_flat)\n",
    "#     # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "#     loss_node2vec = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights,softmax_biases,node_embed,train_context_node_flat, num_sampled, num_nodes))\n",
    "#     loss_node2vec_summary = tf.scalar_summary(\"loss_node2vec\", loss_node2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    # Look up embeddings for words.\n",
    "    node_embed = tf.nn.embedding_lookup(node_embeddings, train_input_node_flat)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss_node2vec = tf.reduce_mean(tf.nn.sampled_softmax_loss(tf.cast(softmax_weights, tf.int32),\n",
    "                                                              tf.cast(softmax_biases, tf.int32),\n",
    "                                                              node_embed,\n",
    "                                                              train_context_node_flat,\n",
    "                                                              num_sampled,\n",
    "                                                              num_nodes))\n",
    "    loss_node2vec_summary = tf.scalar_summary(\"loss_node2vec\", loss_node2vec)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_context_node_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
